{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coding for GANS - learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikuDubenco/code_replications/blob/master/Coding_for_GANS_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNyWB9kTCQtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# credits: Jakub Langr\n",
        "# url: http://jakublangr.com/gans-code.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE1kAv1JDzPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first check the python version\n",
        "\n",
        "import sys\n",
        "\n",
        "if sys.version_info < (3,4):\n",
        "  print('You are running an older version of Python!\\n\\n',\n",
        "       'You should consider updating to Python 3.4.0 or',\n",
        "       'higher as the libraries build for this course',\n",
        "       'have only been tested in Python 3.4 and higher.\\n')\n",
        "  print('Try installing the Python 3.5 version of anaconda',\n",
        "       'and then restart `jupyter notebook`:\\n',\n",
        "       'https://www.continuum.io/downloads\\n\\n')\n",
        "  \n",
        "# now get necessary libraries\n",
        "try:\n",
        "  import os\n",
        "  import pandas as pd\n",
        "  import pickle\n",
        "  import tflearn\n",
        "  from joblib import Parallel, delayed\n",
        "  import random\n",
        "  import multiprocessing\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  from skimage.tranform import resize\n",
        "  from skimage import data\n",
        "  from scipy.misc import imresize\n",
        "  from scipy.ndimage.filters import gaussian_filter\n",
        "  import IPython.display as ipud\n",
        "  import tensorflow as tf\n",
        "  from libs import utils, datasets, dataset_utils, nb_utils\n",
        "except ImportError as e:\n",
        "  print(e)\n",
        "  print('Make sure you have started notebook in the same direction',\n",
        "       'as the provided zip file which includes the `libs` folder',\n",
        "       'and the file `utils.py` inside of it. You will Not be able',\n",
        "       'to complete this assigment unless you restart jupyter',\n",
        "       'notebook inside the directory created by extracting',\n",
        "       'the zip file or cloning the github repo.')\n",
        "  print(e)\n",
        "  \n",
        "# we'll tell matplotlib to inline any drawn figures like so:\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oKnoJNYHtvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = CV.get_celeb_vargan_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JulFQOEH14V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = g.get_tensor_by_name('net/x:0')\n",
        "Z = g.get_tensor_by_name('net/encoder/variational/z:0')\n",
        "G = g.get_tensor_by_name('net/generator/x_tilde:0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m6OiwboITMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = sorted(datasets.CELEB())\n",
        "img_i = 20\n",
        "img = plt.imread(files[img_i])\n",
        "plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXnufJ_gIkv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features_for(label='Bald', has_label=True, n_imgs=50):\n",
        "  # helper function to obtain labels and then preprocessing and returning\n",
        "  # a vector for the seeding function for GAN\n",
        "  # basically figures out the embedding for a particular attribute\n",
        "  label_i = net['labels'].index(label)\n",
        "  label_idxs = np.where(net['attributes'][:, label_i] == has_label)[0]\n",
        "  label_idxs = np.random.permutation(label_idxs)[:n_imgs]\n",
        "  imgs = [plt.imread(files[img_i])[..., :3] for img_i in label_idxs]\n",
        "  preprocessed = np.array([CV.preprocess(img_i) for img_i in imgs])\n",
        "  zs = sess.run(Z, feed_dict={X: preprocessed})\n",
        "  return np.mean(zs, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUUFuRU7Kc_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_ganerate_data(num_iter=20000, imgs=15):\n",
        "  # generate 2*(number of iter) images\n",
        "  # adding random number of pictures for each synthesis (to increase variation)\n",
        "  # returns list of [Male, Female] * num_iter images\n",
        "  generate_images = []\n",
        "  \n",
        "  for i in range(num_iter):\n",
        "    \n",
        "    n_imgs = random.choice(range(imgs-10, imgs+10))\n",
        "    \n",
        "    z1 = get_features_for('Male', True, n_imgs=n_imgs)\n",
        "    z2 = get_features_for('Male', False, n_imgs=n_imgs)\n",
        "    \n",
        "    notmale_vector = z2 - z1\n",
        "    amt = np.linspace(0, 1, 2)\n",
        "    zs = np.array([z1 + notmale_vector * amt_i for amt_i in amt])\n",
        "    g = sess.run(G, feed_dict={Z : zs})\n",
        "    \n",
        "    generate_images.append(g[0])\n",
        "    generate_images.append(g[1])\n",
        "    \n",
        "    if i % 1000 == 0:\n",
        "      print('Iteration number: {}'.format(i))\n",
        "      \n",
        "  return generate_images\n",
        "\n",
        "generated_data = gan_generate_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D5euUBzM9hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [0, 1] * 20000\n",
        "generated_data = np.array(generated_data)\n",
        "generated_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fBzzra-NTHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from libs import vgg16, inception, i2v\n",
        "\n",
        "net = vgg16.get_vgg_face_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF_RUztBQ_Pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transferred_predictions(img):\n",
        "  #gets an image (`np.array`) as an input outputs net's final layer predictions\n",
        "  results = []\n",
        "  \n",
        "  # grab the tensor defining the input to the network\n",
        "  x = g.get_tensor_by_name(names[0] + ':0')\n",
        "  \n",
        "  # and grab the tensor defining the softmax layer of the network\n",
        "  softmax = g.get_tensor_by_name(name[-2] + ':0')\n",
        "  \n",
        "  with tf.Session(grath=g) as sess, g.device('/cpu:0'):\n",
        "    # remember from the lecture that we have to set the dropout\n",
        "    # 'keep probability' to 1.0\n",
        "    res = softmax.eval(feed_dict={x: img})  # not using dropout here\n",
        "           # 'net/dropout_1/random_uniform:0': [[1.0] * 4096]\n",
        "           # 'nat/dropout/randon_uniform:0': [[1.0] * 4096]\n",
        "    test_array = res.argsort()[-5:][::-1].flatten()\n",
        "    result = ([(res.flatten()[int(idx)],\n",
        "                 net['label'][int(idx)]) for idx in test_array])\n",
        "    \n",
        "    result = pd.DataFrame(results, columns=['score', 'label']) # .sort(columns='score')\n",
        "    \n",
        "    results.append(result.score)\n",
        "    \n",
        "  return results\n",
        "\n",
        "def transferred_df(generated_data):\n",
        "  # does the preprocessing of the `list` of generated_data and outputs `list` of predictions\n",
        "  results = []\n",
        "  \n",
        "  for i in range(len(generated_data)):\n",
        "    img = imresize(generated_data[i], size=(224,224,3))\n",
        "    img = net['preprocess'](img)[np.newaxis]\n",
        "    result = transferred_predictions(img)\n",
        "    results.append(result)\n",
        "    \n",
        "    if i % 1000 == 0:\n",
        "      print('Current image id {}'.format(i))\n",
        "      \n",
        "  return results\n",
        "\n",
        "\n",
        "def parallel_transfer_eval(generated_data):\n",
        "  # returns parallely executed `transferred_df` using first split (fs), second ss and ts as divisors\n",
        "  pool = multiprocessing.Pool(4)\n",
        "  fs = int(len(generated_data)/4)\n",
        "  ss = int(2 * len(generated_data)/4)\n",
        "  ts = int(3 * len(generated_data)/4)\n",
        "  target = generated_data[:fs], generated_data[fs:ss], generated_data[ss:ts], generated_data\n",
        "  results = pool.map(transferred_df, zip(target))\n",
        "  # results = Parallel(n_jobs=4)(delayed(transferred_df)(img) for img in generated_data)\n",
        "  \n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnNi0t1W1BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "# train-test for proper evaluation\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y)\n",
        "\n",
        "tflearn.init_graph(num_cores=8, gpu_memory_fraction=.5)\n",
        "\n",
        "# set up the network\n",
        "net = tflearn.input_data(shape=[None, 2623])\n",
        "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
        "net = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# train\n",
        "model = tflearn.DNN(net)\n",
        "model.fit(generated_data, labels, validation_set=train_X)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# reshape labels so that they match what the network expects\n",
        "labels = ['Male', 'Female'] * 10000\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels)\n",
        "labels = encoder.transform(labels)\n",
        "labels = np_utils.to_categorical(labels)\n",
        "labels.shape\n",
        "\n",
        "test_imgs = np.array([CV.preprocess(plt.imread(file)) for file in files[:100]])\n",
        "test_imgs.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZE7aSWAZRfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "labels = [0, 1] * 10000\n",
        "\n",
        "feature_columns = [tf.contrib.layers.real_valued_column('', dimension=2623)]\n",
        "\n",
        "classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n",
        "                                            hidden_units=[2623, 512],\n",
        "                                            gradient_clip_norm=.01,\n",
        "                                            optimizer=tf.train.AdamOptizer(learning_rate=.1),\n",
        "                                            n_classes=2)\n",
        "                                            # model_dir='./model')\n",
        "  \n",
        "# fit model\n",
        "classifier.fit(x=array, y=labels, batch_size=256, steps=10000)\n",
        "\n",
        "# evaluate accuracy\n",
        "test_labels = np.array([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
        "                        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
        "                        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
        "                        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
        "                        0, 0, 0, 0, 0, 0, 0, 0])\n",
        "\n",
        "# test_array = np.array([[res[0] for res in result] for result in test_array])\n",
        "\n",
        "accuracy_score = classifier.evaluate(x=test_array, y=test_labels)['accuracy']\n",
        "print('Accuracy: {0:f}'.format(accuracy_score))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}