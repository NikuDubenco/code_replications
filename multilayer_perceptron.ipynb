{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multilayer perceptron.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikuDubenco/code_replications/blob/master/multilayer_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8-M0PFsIzY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# credits Python machine learning, S. Raschka, V. Mirjalili / book p.396\n",
        "# multilayer perceptron \n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "class NeuralNetMLP(object):\n",
        "  '''\n",
        "  feedforward neural nerwork / Multi-layer perceptron classifier.\n",
        "  \n",
        "  parameters\n",
        "  ----------\n",
        "  n_hidden: int (default: 30)\n",
        "      Number of hidden units.\n",
        "      \n",
        "  12: float (default: 0.)\n",
        "      Lambda value for L2-regularization.\n",
        "      No regularization if L2=0. (default)\n",
        "  \n",
        "  epochs: int(default:100)\n",
        "      Number of passes over the training set.\n",
        "  \n",
        "  eta: float (default: .001)\n",
        "      Learning rate.\n",
        "  \n",
        "  shuffle: bool (default: True)\n",
        "      Shuffle training data every epoch\n",
        "      if True to prevent circles.\n",
        "      \n",
        "  minibatch_size: int (default: 1)\n",
        "      Number of training samples per minibatch.\n",
        "      \n",
        "  seed: int (default: None)\n",
        "      Random seed for initializing weights and shuffling.\n",
        "      \n",
        "      \n",
        "  Attributes\n",
        "  ----------\n",
        "  eval_ : dict\n",
        "      Dictionary collecting the cost, training accuracy, and validation accuracy\n",
        "      for each epoch during training.\n",
        "      \n",
        "  '''\n",
        "  def __init__(self, n_hidden=30, l2=0., epochs=100, eta=.001, shuffle=True,\n",
        "              minibatch_size=1, seed=None):\n",
        "    \n",
        "    self.random = np.random.RandomState(seed)\n",
        "    self.n_hidden = n_hidden\n",
        "    self.l2 = l2\n",
        "    self.epochs = epochs\n",
        "    self.eta = eta\n",
        "    self.shuffle = shuffle\n",
        "    self.minibatch_size = minibatch_size\n",
        "    \n",
        "  def _onehot(self, y, n_classes):\n",
        "    '''\n",
        "    encode labels into one-hot representation\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    \n",
        "    y: array, shape = [n_samples]\n",
        "       target values.\n",
        "       \n",
        "    returns\n",
        "    -------\n",
        "    cost: float\n",
        "        regularized cost\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    z_h, a_h, z_out, a_out = self._forward(X)\n",
        "    y_pred - np.argmax(z_out, axis=1)\n",
        "    return y_pred\n",
        "  \n",
        "  def fit(self, X_train, y_train, X_val, y_val):\n",
        "    '''\n",
        "    learn weights from training data.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train: array, shape = [n_samples, n_features]\n",
        "        input layer with original features.\n",
        "    y_train: array, shape = [n_samples]\n",
        "        target class labels.\n",
        "    X_val: array, shape = [n_samples, n_features]\n",
        "        sample features for validation during training\n",
        "    y_val: array, shape = [n_samples]\n",
        "        sample labels for validation during training\n",
        "        \n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    self\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    n_output = np.unique(y_train).shape[0]  # no. of classlabels\n",
        "    \n",
        "    n_features = X_train.shape[1]\n",
        "    \n",
        "    #############################\n",
        "    # Weight initialization\n",
        "    #############################\n",
        "    \n",
        "    # Weight for input -> hidden\n",
        "    self.b_h = np.zeros(self.n_hidden)\n",
        "    self.w_h = self.random.normal(loc=0.0, scale=.1, size=(n_features, n_output))\n",
        "    \n",
        "    epoch_strlen = len(str(self.epochs))  # for progr. format.\n",
        "    self.eval_ = {'cost':[], 'train_acc':[], 'val_acc':[]}\n",
        "    \n",
        "    y_train_enc = self._onehot(y_train, n_output)\n",
        "    \n",
        "    # iterate over training epochs\n",
        "    for i in range(self.epochs):\n",
        "      #iterate over minibatches\n",
        "      indices = np.arange(X_train.shape[0])\n",
        "      \n",
        "      if self.shuffle:\n",
        "        self.random.shuffle(indices)\n",
        "        \n",
        "      for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, \n",
        "                             self.minibatch_size):\n",
        "        batch_idx = indices[start_idx: start_idx + self.minibatch_size]\n",
        "        \n",
        "        # forward propagation\n",
        "        z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
        "        \n",
        "        ######################\n",
        "        # Backpropagation\n",
        "        ######################\n",
        "        \n",
        "        # [n_samples, n_classlabels]\n",
        "        sigma_out = a_out - y_train_enc[batch_idx]\n",
        "        \n",
        "        # [n_samples, n_hidden]\n",
        "        sigmoid_derivative_h = a_h * (1. - a_h)\n",
        "        \n",
        "        # [n_samples, n_classlabels] dot [n_classlabels, n_hidden] -> [n_samples, n_hidden]\n",
        "        sigma_h = (np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h)\n",
        "        \n",
        "        # [n_features, n_samples] dot [n_samples, n_hidden] -> [n_features, n_hidden]\n",
        "        grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n",
        "        grad_b_h = np.sum(sigma_h, axis=0)\n",
        "        \n",
        "        # [n_hidden, n_samples] dot [n_samples, n_classlabels] -> [n_hidden, n_classlabels]\n",
        "        grad_w_out = np.dot(a_h.T, sigma_out)\n",
        "        grad_b_out = np.sum(sigma_out, axis=0)\n",
        "        \n",
        "        # regularization and weight updates\n",
        "        delta_w_h = (grad_w_h + self.l2 * self.w_h)\n",
        "        delta_b_h = grad_b_h  # bias is not regularized\n",
        "        self.w_h -= self.eta * delta_w_out\n",
        "        self.b_h -= self.eta * delta_b_out\n",
        "        \n",
        "      ####################\n",
        "      # Evaluation\n",
        "      ###################\n",
        "\n",
        "      # evaluation after each epoch during training\n",
        "      z_h, a_h, z_out, a_out = self._forward(X_train)\n",
        "\n",
        "      cost = self._compute_cost(y_enc=y_train_enc, output=a_out)\n",
        "\n",
        "      y_train_pred = self.predict(X_train)\n",
        "      y_val_pred = self.predict(X_val)\n",
        "\n",
        "      train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) / X_train.shape[0])\n",
        "\n",
        "      val_acc = ((np.sum(y_val == y_val_pred)).astype(np.float) / X_val.shape[0])\n",
        "\n",
        "      sys.stderr.write('\\r%0*d/%d | Cost: %.2f | Train/Val Acc: %.2f%%/%.2f%% '\n",
        "                       %\n",
        "                       (epoch_strlen, i+1, self.epochs, cost, train_acc * 100, \n",
        "                       val_acc * 100))\n",
        "\n",
        "      sys.stderr.flush()\n",
        "\n",
        "      self.eval_['cost'].append(cost)\n",
        "      self.eval_['train_acc'].append(train_acc)\n",
        "      self.eval_['val_acc'].append(val_acc)\n",
        "      \n",
        "    return self\n",
        "        \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}